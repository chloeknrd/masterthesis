{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "import pickle\n",
    "from youtube_transcript_api import YouTubeTranscriptApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sets up the necessary parameters for the API.\n",
    "\n",
    "To prevent unrelated videos being gathered alongside before the Fridays for Future movement, relevant terms are added to the query after their emergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_server = \"youtube\"\n",
    "api_vers = \"v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Key\n",
    "with open('key.txt') as key:\n",
    "    DEV_KEY = [line.strip() for line in key]\n",
    "DEV_KEY = DEV_KEY[0]\n",
    "DEV_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"klimawandel|umweltschutz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube = googleapiclient.discovery.build(\n",
    "    serviceName = api_server,\n",
    "    version = api_vers, \n",
    "    developerKey = DEV_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sets up the dictionary to temporarily save the scraping results, before adding them to the dataframe within the main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_response = {\n",
    "    'vid_id': [],\n",
    "    'Publish_date': [],\n",
    "    'Channel_ID': [],\n",
    "    'Title': [],\n",
    "    'Description': [],\n",
    "    'Channel_Title': []\n",
    "}\n",
    "date_begin = dt.strptime('2015-01-01', \"%Y-%m-%d\")\n",
    "date_end = date_begin + timedelta(days=7)\n",
    "date_finish = dt.strptime('2019-12-31', \"%Y-%m-%d\")\n",
    "date_split = dt.strptime('2018-08-20', '%Y-%m-%d')\n",
    "next_token = \"\"\n",
    "#date_split = dt.strptime(\"2017-08-20\", '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are used to ensure that the dates are in string format for the data queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date_begin(date_str):\n",
    "    date_str = date_str.strftime(\"%Y-%m-%d\") + 'T23:59:59Z'\n",
    "    return date_str\n",
    "\n",
    "def convert_date_end(date_str):\n",
    "    date_str = date_str.strftime(\"%Y-%m-%d\") + 'T00:00:00Z'\n",
    "    return date_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next three code fields are used when scraping at a later date after exceeding the daily quota of 10.000 queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_data = pd.read_pickle('./results/yt_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = yt_data['Publish_date'].tolist()\n",
    "for i in range(len(date_list)):\n",
    "    date_list[i] = dt.strptime(date_list[i], \"%Y-%m-%dT%H:%M:%SZ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = max(date_list)\n",
    "date_begin = last_date\n",
    "date_end = date_begin + timedelta(days=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a first query to setup the format and the nextPageToken variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = youtube.search().list(\n",
    "        part=\"id, snippet\",\n",
    "        type = \"video\",\n",
    "        publishedAfter = convert_date_begin(date_str=date_begin),\n",
    "        publishedBefore = convert_date_end(date_str=date_end),\n",
    "        relevanceLanguage = 'de',\n",
    "        q = query,\n",
    "        maxResults = 50,\n",
    "        fields = \"nextPageToken, pageInfo.totalResults, items(id(videoId), snippet(publishedAt, channelId, channelTitle, title, description))\"\n",
    ").execute()\n",
    "if \"nextPageToken\" in request:\n",
    "    next_token = request['nextPageToken']\n",
    "else:\n",
    "    date_begin = date_begin + timedelta(days=7)\n",
    "    date_end = date_end + timedelta(days=7)\n",
    "\n",
    "for i in range(len(request['items'])):\n",
    "\n",
    "    vid_id = request['items'][i]['id']['videoId']\n",
    "    Publish_Date = request['items'][i]['snippet']['publishedAt']\n",
    "    Channel_ID = request['items'][i]['snippet']['channelId']\n",
    "    Title = request['items'][i]['snippet']['title']\n",
    "    Description = request['items'][i]['snippet']['description']\n",
    "    Channel_Title = request['items'][i]['snippet']['channelTitle']\n",
    "    video_response['vid_id'].append(vid_id)\n",
    "    video_response['Publish_date'].append(Publish_Date)\n",
    "    video_response['Channel_ID'].append(Channel_ID)\n",
    "    video_response['Title'].append(Title)\n",
    "    video_response['Description'].append(Description)\n",
    "    video_response['Channel_Title'].append(Channel_Title)\n",
    "#print(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loop is the main approach to automatically gather data until either the daily quota is exceeded or all results have been scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    if next_token:\n",
    "        if date_begin >= date_split:\n",
    "            query = \"klimawandel|fridays for future|umweltschutz|greta thunberg\"\n",
    "        request = youtube.search().list(\n",
    "            part=\"id, snippet\",\n",
    "            type = \"video\",\n",
    "            publishedAfter = convert_date_begin(date_str=date_begin),\n",
    "            publishedBefore = convert_date_end(date_str=date_end),\n",
    "            relevanceLanguage = 'de',\n",
    "            q = query,\n",
    "            maxResults = 50,\n",
    "            fields = \"nextPageToken, items(id(videoId), snippet(publishedAt, channelId, channelTitle, title, description))\",\n",
    "            pageToken = next_token\n",
    "        ).execute()\n",
    "    else:\n",
    "        request = youtube.search().list(\n",
    "            part=\"id, snippet\",\n",
    "            type = \"video\",\n",
    "            publishedAfter = convert_date_begin(date_str=date_begin),\n",
    "            publishedBefore = convert_date_end(date_str=date_end),\n",
    "            relevanceLanguage = 'de',\n",
    "            q = query,\n",
    "            maxResults = 50,\n",
    "            fields = \"nextPageToken, pageInfo.totalResults, items(id(videoId), snippet(publishedAt, channelId, channelTitle, title, description))\"\n",
    "        ).execute()    \n",
    "\n",
    "    for i in range(len(request['items'])):\n",
    "\n",
    "        vid_id = request['items'][i]['id']['videoId']\n",
    "        Publish_Date = request['items'][i]['snippet']['publishedAt']\n",
    "        Channel_ID = request['items'][i]['snippet']['channelId']\n",
    "        Title = request['items'][i]['snippet']['title']\n",
    "        Description = request['items'][i]['snippet']['description']\n",
    "        Channel_Title = request['items'][i]['snippet']['channelTitle']\n",
    "        video_response['vid_id'].append(vid_id)\n",
    "        video_response['Publish_date'].append(Publish_Date)\n",
    "        video_response['Channel_ID'].append(Channel_ID)\n",
    "        video_response['Title'].append(Title)\n",
    "        video_response['Description'].append(Description)\n",
    "        video_response['Channel_Title'].append(Channel_Title)\n",
    "    \n",
    "    if 'nextPageToken' in request:\n",
    "        next_token = request['nextPageToken']\n",
    "    else:\n",
    "        date_begin = date_begin + timedelta(days=7)\n",
    "        date_end = date_end + timedelta(days=7)\n",
    "        next_token = \"\"\n",
    "\n",
    "    if date_begin > date_finish:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is used after the first scraping attempt, to save the current progress and to setup the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erstes Aufsetzen \n",
    "yt_data = pd.DataFrame(data = video_response)\n",
    "yt_data.to_pickle('./results/yt_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this code box is used after the first approach, when there are results already present within the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(data=video_response)\n",
    "yt_data = pd.concat([yt_data, results_df])\n",
    "yt_data.to_pickle('./results/yt_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this box is used to load the necessary data, either for the following steps or for repeated scraping attemtps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/yt_data.pkl', 'rb') as data:\n",
    "    yt_data = pickle.load(file=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the YouTube Transcript package uses only the video IDs, they are extracted, so that it is possible to iterate through the list. While the package has a function that allows to completely insert a list of video IDs, there is no failsafe present within that function if a video has no transcripts.\n",
    "\n",
    "Thus, a \"try\"-\"except\" approach is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_list = yt_data['vid_id'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_capt = []\n",
    "\n",
    "for i in range(len(video_list)):\n",
    "    try:\n",
    "        req_cap = YouTubeTranscriptApi.get_transcript(video_id=video_list[i], languages=['de'])\n",
    "    except:\n",
    "        req_cap = float(\"nan\")\n",
    "    req_tuple = (video_list[i], req_cap)\n",
    "    list_capt.append(req_tuple)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two boxes are to provide code for saving and laoding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list_capt.pkl', 'wb') as file:\n",
    "    pickle.dump(list_capt, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('list_capt.pkl', 'rb') as file:\n",
    "    list_capt = pickle.load(file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of videos without a transcript is used to filter out videos without a transcript, as these can't be used in the further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_df = pd.DataFrame(list_capt, columns=['Vid_Id', 'Trans'])\n",
    "\n",
    "cap_df_clean = cap_df.dropna()\n",
    "cap_df_clean = cap_df_clean.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transcripts are stored in a dictionary alongside other informations like the start and end time of the lines in the transcript. This additional data needs to be removed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_df_clean.loc[:,'Trans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 'text'\n",
    "trans_list = []\n",
    "for i in range(len(cap_df_clean.loc[:,'Trans'])):\n",
    "    res = [d.get(k) for d in cap_df_clean.loc[i,'Trans'] if k in d]\n",
    "    res = ' '.join(res)\n",
    "    res_tup = (cap_df_clean.loc[i, 'Vid_Id'], res)\n",
    "    trans_list.append(res_tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the transcripts are stored in a dataframe and then saved as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trans_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m script_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mtrans_list\u001b[49m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVid_Id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrans\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trans_list' is not defined"
     ]
    }
   ],
   "source": [
    "script_df = pd.DataFrame(trans_list, columns=['Vid_Id', 'Trans'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('transcripts.pkl', 'wb') as file:\n",
    "    pickle.dump(script_df, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('transcripts.pkl', 'rb') as file:\n",
    "    script_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_data_filter = pd.merge(script_df, yt_data, left_on='Vid_Id', right_on='vid_id', how='inner')\n",
    "yt_data_filter = yt_data_filter.drop(columns=['vid_id'])\n",
    "yt_data_filter = yt_data_filter.reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gather the channel category, a similar approach for the video IDs and transcripts is used, except here, the list of IDs already exists due to the first YouTube API scraping process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_unique = yt_data_filter['Channel_ID'].unique()\n",
    "channels_list = channels_unique.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_resp = {\n",
    "    'Channel_ID': [],\n",
    "    'TopicDetails': [],\n",
    "}\n",
    "\n",
    "for i in range(len(channels_list)):\n",
    "    request = youtube.channels().list(\n",
    "        part=\"id,topicDetails\",\n",
    "        id=channels_list[i]\n",
    "    ).execute()\n",
    "\n",
    "    if 'items' in request:\n",
    "        for j in range(len(request['items'])):\n",
    "            channel_id = channels_list[i]\n",
    "            if 'topicDetails' in request['items'][0]:\n",
    "                topicDets = request['items'][j]['topicDetails']\n",
    "                if 'topicCategories' in topicDets:\n",
    "                    topics = topicDets['topicCategories'][0]\n",
    "                else:\n",
    "                    topics = 'NaN'\n",
    "            else:\n",
    "                topics = 'NaN'\n",
    "    else:\n",
    "        channel_id = channels_list[i]\n",
    "        topics = 'NaN'\n",
    "\n",
    "    channel_resp['Channel_ID'].append(channel_id)\n",
    "    channel_resp['TopicDetails'].append(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_cats = pd.DataFrame(data=channel_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"channel_cats.pkl\", 'wb') as file:\n",
    "    pickle.dump(channel_cats, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('channel_cats.pkl', 'rb') as file:\n",
    "    cats_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categories are stored as Wikipedia links. Thus, the first part of the link needs to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(cats_df)):\n",
    "    if cats_df.loc[i, 'TopicDetails'] != 'NaN':\n",
    "        cats_df.loc[i, 'TopicDetails'] = cats_df.loc[i, 'TopicDetails'].replace('https://en.wikipedia.org/wiki/', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the cleaned data frame is stored in a pickle file as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cats_df.pkl', 'wb') as file:\n",
    "    pickle.dump(cats_df, file=file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
